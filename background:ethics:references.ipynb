{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Project Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "You have the choice of doing either (1) an AI solve a problem style project or (2) run a Special Topics class on a topic of your choice.  If you want to do (2) you should fill out the _other_ proposal for that. This is the proposal description for (1).\n",
    "\n",
    "You will design and execute a machine learning project. There are a few constraints on the nature of the allowed project. \n",
    "- The problem addressed will not be a \"toy problem\" or \"common training students problem\" like 8-Queens or a small Traveling Salesman Problem or similar\n",
    "- If its the kind of problem (e.g., RL) that interacts with a simulator or live task, then the problem will have a reasonably complex action space. For instance, a wupus world kind of thing with a 9x9 grid is definitely too small.  A simulated mountain car with a less complex 2-d road and simplified dynamics seems like a fairly low achievement level.  A more complex 3-d mountain car simulation with large extent and realistic dynamics, sure sounds great!\n",
    "- If its the kind of problem that uses a dataset, then the dataset will have >1k observations and >5 variables. I'd prefer more like >10k observations and >10 variables. A general rule is that if you have >100x more observations than variables, your solution will likely generalize a lot better. The goal of training an unsupervised machine learning model is to learn the underlying pattern in a dataset in order to generalize well to unseen data, so choosing a large dataset is very important.\n",
    "- The project must include some elements we talked about in the course\n",
    "- The project will include a model selection and/or feature selection component where you will be looking for the best setup to maximize the performance of your AI system. Generally RL tasks may require a huge amount of training, so extensive grid search is unlikely to be possible. However expoloring a few reasonable hyper-parameters may still be possible. \n",
    "- You will evaluate the performance of your AI system using more than one appropriate metric\n",
    "- You will be writing a report describing and discussing these accomplishments\n",
    "\n",
    "\n",
    "Feel free to delete this description section when you hand in your proposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Sebastian Modafferi\n",
    "- Chirag Amatya\n",
    "- Michael Chu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This section should be short and clearly stated. It should be a single paragraph <200 words.  It should summarize: \n",
    "- what your goal/problem is\n",
    "- what the data used represents and how they are measured\n",
    "- what you will be doing with the data\n",
    "- how performance/success will be measured"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Reinforcement learning (RL) has become a prominent approach for solving complex decision-making tasks, particularly in environments requiring continuous control and sequential decision-making. The Lunar Lander task, provided by the Gymnasium framework, serves as an effective benchmark for evaluating RL algorithms. This task involves navigating a simulated spacecraft to a safe landing position using thrusters while minimizing fuel consumption. Given the large, continuous state space of the environment, traditional dynamic programming methods are infeasible as they require complete knowledge of the environment's state transitions <a name=\"suttonnote\"></a>[1]. Instead, Monte Carlo (MC) methods, which learn directly from episodic experiences, provide a more suitable alternative, though their reliance on complete episodes for updates results in slower convergence and increased memory demands <a name=\"2\"></a>[2] .\n",
    "\n",
    "Temporal-Difference (TD) learning methods offer a solution to these limitations by updating value estimates at each step rather than waiting for entire episodes to complete. Q-learning, a widely used TD method, learns an optimal policy by iteratively improving state-action values without requiring a full model of the environment <a name=\"3\"></a>[3] \n",
    ". However, in environments with high-dimensional state spaces, tabular representations of Q-values become impractical, necessitating function approximation techniques. Deep Q-Networks (DQN), introduced by DeepMind, leverage deep neural networks to approximate Q-values, enabling learning in complex, high-dimensional environments <a name=\"4\"></a>[4] \n",
    ". This breakthrough demonstrated that deep reinforcement learning could achieve human-level performance in various control tasks, such as playing Atari games.\n",
    "\n",
    "In this project, we explore the use of DQN and Double DQN (DDQN) to optimize landing strategies in the Lunar Lander environment. By evaluating their performance in terms of convergence speed, stability, and policy effectiveness, we aim to gain insights into the effectiveness of deep reinforcement learning in real-world-inspired autonomous control tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The problem we are solving is the [lunar lander task](https://gymnasium.farama.org/environments/box2d/lunar_lander/) provided by the `gymnasium` python package. The task is to land a vehicle powered by three engines onto a randomly-generated surface. The lander is comrpised of two orientation engines, which add angular velocity to rotate the lander, along with one main engine which provides x/y-velocity depending on the orientation. The task begins with the lander in the top-center of the simulation, with a random initial force acting upon it. The lander must then use the three engines to not only correct for the initial force, but to also touchdown near (0,0), such that the two \"legs\" contact the surface. In order to successfully complete the task the lander must land close to (0,0) and must use the engines sparingly, as their is a reward cost for engine usage. The total reward breakdown is listed below, as described on [the docs](https://gymnasium.farama.org/environments/box2d/lunar_lander/).\n",
    "\n",
    "\n",
    "- **Increases** the closer the lander is to the landing pad and **decreases** the further it is.\n",
    "- **Increases** the slower the lander is moving and **decreases** the faster it is moving.\n",
    "- **Decreases** the more the lander is tilted (i.e., the angle is not horizontal).\n",
    "- **Increases** by **10 points** for each leg that is in contact with the ground.\n",
    "- **Decreases** by **0.03 points** for each frame a side engine is firing.\n",
    "- **Decreases** by **0.3 points** for each frame the main engine is firing.\n",
    "\n",
    "Additionally, the episode receives:\n",
    "\n",
    "- **-100 points** for crashing.\n",
    "- **+100 points** for landing safely.\n",
    "\n",
    "In order to successfully complete the task, the lander must achieve __at least__ 200 points. Additional points will be used to compare performance across multiple agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "You should have a strong idea of what dataset(s) will be used to accomplish this project. \n",
    "\n",
    "If you know what (some) of the data you will use, please give the following information for each dataset:\n",
    "- link/reference to obtain it\n",
    "- description of the size of the dataset (# of variables, # of observations)\n",
    "- what an observation consists of\n",
    "- what some critical variables are, how they are represented\n",
    "- any special handling, transformations, cleaning, etc will be needed\n",
    "\n",
    "If you don't yet know what your dataset(s) will be, you should describe what you desire in terms of the above bullets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Provide enough detail (e.g., algorithmic description and/or theoretical properties) to convince us that your solution is applicable. Why might your solution work? Make sure to describe how the solution will be tested.  \n",
    "\n",
    "If you know details already, describe how (e.g., library used, function calls) you plan to implement the solution in a way that is reproducible.\n",
    "\n",
    "If it is appropriate to the problem statement, describe a benchmark model<a name=\"sota\"></a>[<sup>[3]</sup>](#sotanote) against which your solution will be compared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Propose at least one evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model. The evaluation metric(s) you propose should be appropriate given the context of the data, the problem statement, and the intended solution. Describe how the evaluation metric(s) are derived and provide an example of their mathematical representations (if applicable). Complex evaluation metrics should be clearly defined and quantifiable (can be expressed in mathematical or logical terms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ethics and privacy concerns in the Lunar Lander project primarily revolve around reinforcement learning risks, data handling, and potential real-world implications. Since the model learns through trial and error, there is a risk of unsafe exploration if not properly controlled, which could encourage reckless decision-making in real-world applications. Additionally, while this project operates in a simulated environment, lessons from it could influence autonomous landing systems, requiring careful consideration of safety and bias. From a data perspective, ensuring fair training practices, avoiding unintended biases, and minimizing environmental impact from high computational resource usage are key concerns. Implementing fairness-aware learning, optimizing resource use, and maintaining transparency in model behavior help mitigate these risks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put things here that cement how you will interact/communicate as a team, how you will handle conflict and difficulty, how you will handle making decisions and setting goals/schedule, how much work you expect from each other, how you will handle deadlines, etc...\n",
    " - Communicate delays or updates on progress as soon as possible.\n",
    " - Attend meetings on time - or notify of delays prior.\n",
    " - Be respectful of other's opinions.\n",
    " - Complete your assigned work with integrity and to your best ability.\n",
    " - Share bugs that could be common across other tasks - improve eachother's ability to work efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace this with something meaningful that is appropriate for your needs. It doesn't have to be something that fits this format.  It doesn't have to be set in stone... \"no battle plan survives contact with the enemy\". But you need a battle plan nonetheless, and you need to keep it updated so you understand what you are trying to accomplish, who's responsible for what, and what the expected due dates are for each item.\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/11  |  2 PM | Brainstorm topics/questions (all)  | Communication methods - Project Task - Separation of work for Proposal | \n",
    "| 2/13  |  6 PM | Completed proposal assigned sections | discuss proposal submission |\n",
    "| 2/18  |  2 PM | Found individual approaches - Basecode for working with gynasium shared and completed | Discuss valid approaches - Finalize timeline | \n",
    "| 2/25  | 2 PM  | Progress on individual approaches | Assign each group member to their preferred approach - Begin developing |\n",
    "| 2/14  | 2 PM  | TBD | TBD   |\n",
    "| 2/23  | 2 PM  | TBD | TBD   |\n",
    "| 3/4  | 2 PM  | Near Completion for code-side of Project | Discuss writing strategies - compare preformances |\n",
    "| 3/11 | 2 PM | TBD | TBD   |\n",
    "| 3/19  | Before 11:59 PM  | Whole Project! | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "\n",
    "1. Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction (2nd ed.). MIT Press.\n",
    "\n",
    "2. Watkins, C. J. C. H., & Dayan, P. (1992). Q-learning. Machine Learning, 8(3-4), 279-292. https://doi.org/10.1007/BF00992698 \n",
    "\n",
    "3. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., & Hassabis, D. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529-533. https://doi.org/10.1038/nature14236 \n",
    "\n",
    "4. Russell, S., & Norvig, P. (2020). Artificial Intelligence: A Modern Approach (4th ed.). Pearson. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **References**\n",
    "<a name=\"1\"></a>[1] Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction* (2nd ed.). MIT Press.  \n",
    "<a name=\"2\"></a>[2] \n",
    "<a name=\"3\"></a>[3] \n",
    "<a name=\"4\"></a>[4] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 07:03:16) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
